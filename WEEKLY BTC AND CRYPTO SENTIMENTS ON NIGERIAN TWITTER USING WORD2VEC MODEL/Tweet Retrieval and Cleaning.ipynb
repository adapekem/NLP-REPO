{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the start time the program started running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "starttime = datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "#nltk.downloader.download('vader_lexicon')\n",
    "import pycountry\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authentication.\n",
    "Apikey is a py file in my local directory that has my authentication details. To request for twitter Api key visit https://developer.twitter.com/en/docs/tutorials/step-by-step-guide-to-making-your-first-request-to-the-twitter-api-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apikey import api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve tweets\n",
    "\n",
    "You can also set the number of tweets you want to retrieve here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Nigeria_geocode = \"9.081999,8.675277,100km\"\n",
    "keyword = 'BTC OR bitcoin OR crypto'\n",
    "noOfTweet = 3000\n",
    "#tweepy.Cursor(api.search, q=keyword, geocode = Nigeria_geocode).items(noOfTweet)\n",
    "tweets = tweepy.Cursor(api.search, q=keyword).items(noOfTweet)\n",
    "\n",
    "tweet_list = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    if \"RT\" not in str(tweet) or re.match(r\"[0-9]*$\", str(tweet)) == False:\n",
    "        \n",
    "        tweet_list.append(tweet.text)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put tweets in a dataframe\n",
    "rename the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "\n",
    "tweet_list['Tweets'] = tweet_list[0]\n",
    "del tweet_list[0]\n",
    "tweet_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the tweets \n",
    "remove hastags, symbols and hyperlinks, then convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove hyperlinks\n",
    "remove_hyperlinks= lambda x: re.sub(r'https?:\\/\\/.*[\\r\\n]*','', x)\n",
    "\n",
    "    \n",
    "#Remove hastags\n",
    "#Only removing the hash # sign from the word\n",
    "remove_hashtag = lambda x: re.sub(r'#','', x)\n",
    "    \n",
    "#Only underscore sign from the word\n",
    "remove_underscore = lambda x: re.sub(r'_','', x)\n",
    "\n",
    "tweet_list['Tweets']= tweet_list['Tweets'].map(remove_hyperlinks).map(remove_hashtag).map(remove_underscore)\n",
    "   \n",
    "#convert to lower\n",
    "tweet_list['Tweets'] = tweet_list['Tweets'].str.lower()\n",
    "\n",
    "#Drop duplicates\n",
    "tweet_list.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "tweet_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export and append tweets to csv file \n",
    "by doing this we're building our dataset and increasing the corpus for our training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export list to csv, run this program daily and append to the csv to increase the training dataset#\n",
    "\n",
    "tweet_list.to_csv('train_tweets.csv', mode='a', header=False, index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate runtime of entire program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this shows runtime\n",
    "datetime.now() - starttime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
