{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon review dataset 2018\n",
    "You can reach out the complete dataset at: http://deepyeti.ucsd.edu/jianmo/amazon/index.html\n",
    "\n",
    "We are not in any position to offer any license on the data. Please cite the following paper if you use the data in any way:\n",
    "\n",
    "Justifying recommendations using distantly-labeled reviews and fined-grained aspects\n",
    "Jianmo Ni, Jiacheng Li, Julian McAuley\n",
    "Empirical Methods in Natural Language Processing (EMNLP), 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json #the file is json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read json into a list\n",
    "f = open('Luxury_Beauty.json', 'r')\n",
    "dataset = []\n",
    "for line in f:\n",
    "    dataset.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>06 15, 2010</td>\n",
       "      <td>A1Q6MUU0B2ZDQG</td>\n",
       "      <td>B00004U9V2</td>\n",
       "      <td>D. Poston</td>\n",
       "      <td>I bought two of these 8.5 fl oz hand cream, an...</td>\n",
       "      <td>dispensers don't work</td>\n",
       "      <td>1276560000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "      <td>01 7, 2010</td>\n",
       "      <td>A3HO2SQDCZIE9S</td>\n",
       "      <td>B00004U9V2</td>\n",
       "      <td>chandra</td>\n",
       "      <td>Believe me, over the years I have tried many, ...</td>\n",
       "      <td>Best hand cream ever.</td>\n",
       "      <td>1262822400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>04 18, 2018</td>\n",
       "      <td>A2EM03F99X3RJZ</td>\n",
       "      <td>B00004U9V2</td>\n",
       "      <td>Maureen G</td>\n",
       "      <td>Great hand lotion</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1524009600</td>\n",
       "      <td>{'Size:': ' 3.5 oz.'}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>04 18, 2018</td>\n",
       "      <td>A3Z74TDRGD0HU</td>\n",
       "      <td>B00004U9V2</td>\n",
       "      <td>Terry K</td>\n",
       "      <td>This is the best for the severely dry skin on ...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1524009600</td>\n",
       "      <td>{'Size:': ' 3.5 oz.'}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>04 17, 2018</td>\n",
       "      <td>A2UXFNW9RTL4VM</td>\n",
       "      <td>B00004U9V2</td>\n",
       "      <td>Patricia Wood</td>\n",
       "      <td>The best non- oily hand cream ever. It heals o...</td>\n",
       "      <td>I always have a backup ready.</td>\n",
       "      <td>1523923200</td>\n",
       "      <td>{'Size:': ' 3.5 oz.'}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574623</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>03 20, 2017</td>\n",
       "      <td>AHYJ78MVF4UQO</td>\n",
       "      <td>B01HIQEOLO</td>\n",
       "      <td>Lori Fox</td>\n",
       "      <td>Great color and I prefer shellac over gel</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1489968000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574624</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>10 26, 2016</td>\n",
       "      <td>A1L2RT7KBNK02K</td>\n",
       "      <td>B01HIQEOLO</td>\n",
       "      <td>Elena</td>\n",
       "      <td>Best shellac I have ever used.  It doesn't tak...</td>\n",
       "      <td>Best shellac I have ever used</td>\n",
       "      <td>1477440000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574625</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>09 30, 2016</td>\n",
       "      <td>A36MLXQX9WPPW9</td>\n",
       "      <td>B01HIQEOLO</td>\n",
       "      <td>Donna D. Harris</td>\n",
       "      <td>Great polish and beautiful color!!</td>\n",
       "      <td>Great polish!</td>\n",
       "      <td>1475193600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>12 5, 2016</td>\n",
       "      <td>A23DRCOMC2RIXF</td>\n",
       "      <td>B01HJ2UY0W</td>\n",
       "      <td>Y.Y. Chen</td>\n",
       "      <td>The perfume is good, but the spray head broke ...</td>\n",
       "      <td>Spray head broke off within a month</td>\n",
       "      <td>1480896000</td>\n",
       "      <td>{'Size:': ' 1.7 Fluid Ounce', 'Color:': ' Multi'}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574627</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>01 14, 2017</td>\n",
       "      <td>AJEDVHTLS9P3V</td>\n",
       "      <td>B01HJ2UY1G</td>\n",
       "      <td>ML Shelton</td>\n",
       "      <td>Great fragrance.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1484352000</td>\n",
       "      <td>{'Size:': ' 3.4 Fluid Ounce', 'Color:': ' Multi'}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574628 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall vote  verified   reviewTime      reviewerID        asin  \\\n",
       "0           2.0    3      True  06 15, 2010  A1Q6MUU0B2ZDQG  B00004U9V2   \n",
       "1           5.0   14      True   01 7, 2010  A3HO2SQDCZIE9S  B00004U9V2   \n",
       "2           5.0  NaN      True  04 18, 2018  A2EM03F99X3RJZ  B00004U9V2   \n",
       "3           5.0  NaN      True  04 18, 2018   A3Z74TDRGD0HU  B00004U9V2   \n",
       "4           5.0  NaN      True  04 17, 2018  A2UXFNW9RTL4VM  B00004U9V2   \n",
       "...         ...  ...       ...          ...             ...         ...   \n",
       "574623      5.0  NaN      True  03 20, 2017   AHYJ78MVF4UQO  B01HIQEOLO   \n",
       "574624      5.0  NaN      True  10 26, 2016  A1L2RT7KBNK02K  B01HIQEOLO   \n",
       "574625      5.0  NaN      True  09 30, 2016  A36MLXQX9WPPW9  B01HIQEOLO   \n",
       "574626      1.0    2      True   12 5, 2016  A23DRCOMC2RIXF  B01HJ2UY0W   \n",
       "574627      5.0  NaN      True  01 14, 2017   AJEDVHTLS9P3V  B01HJ2UY1G   \n",
       "\n",
       "           reviewerName                                         reviewText  \\\n",
       "0             D. Poston  I bought two of these 8.5 fl oz hand cream, an...   \n",
       "1               chandra  Believe me, over the years I have tried many, ...   \n",
       "2             Maureen G                                  Great hand lotion   \n",
       "3               Terry K  This is the best for the severely dry skin on ...   \n",
       "4         Patricia Wood  The best non- oily hand cream ever. It heals o...   \n",
       "...                 ...                                                ...   \n",
       "574623         Lori Fox          Great color and I prefer shellac over gel   \n",
       "574624            Elena  Best shellac I have ever used.  It doesn't tak...   \n",
       "574625  Donna D. Harris                 Great polish and beautiful color!!   \n",
       "574626        Y.Y. Chen  The perfume is good, but the spray head broke ...   \n",
       "574627       ML Shelton                                   Great fragrance.   \n",
       "\n",
       "                                    summary  unixReviewTime  \\\n",
       "0                     dispensers don't work      1276560000   \n",
       "1                     Best hand cream ever.      1262822400   \n",
       "2                                Five Stars      1524009600   \n",
       "3                                Five Stars      1524009600   \n",
       "4             I always have a backup ready.      1523923200   \n",
       "...                                     ...             ...   \n",
       "574623                           Five Stars      1489968000   \n",
       "574624        Best shellac I have ever used      1477440000   \n",
       "574625                        Great polish!      1475193600   \n",
       "574626  Spray head broke off within a month      1480896000   \n",
       "574627                           Five Stars      1484352000   \n",
       "\n",
       "                                                    style image  \n",
       "0                                                     NaN   NaN  \n",
       "1                                                     NaN   NaN  \n",
       "2                                   {'Size:': ' 3.5 oz.'}   NaN  \n",
       "3                                   {'Size:': ' 3.5 oz.'}   NaN  \n",
       "4                                   {'Size:': ' 3.5 oz.'}   NaN  \n",
       "...                                                   ...   ...  \n",
       "574623                                                NaN   NaN  \n",
       "574624                                                NaN   NaN  \n",
       "574625                                                NaN   NaN  \n",
       "574626  {'Size:': ' 1.7 Fluid Ounce', 'Color:': ' Multi'}   NaN  \n",
       "574627  {'Size:': ' 3.4 Fluid Ounce', 'Color:': ' Multi'}   NaN  \n",
       "\n",
       "[574628 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert list to pandas dataframe\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall           float64\n",
       "vote               object\n",
       "verified             bool\n",
       "reviewTime         object\n",
       "reviewerID         object\n",
       "asin               object\n",
       "reviewerName       object\n",
       "reviewText         object\n",
       "summary            object\n",
       "unixReviewTime      int64\n",
       "style              object\n",
       "image              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check datatype\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_review</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I bought two of these 8.5 fl oz hand cream, an...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Believe me, over the years I have tried many, ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Great hand lotion Five Stars</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>This is the best for the severely dry skin on ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The best non- oily hand cream ever. It heals o...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        final_review  overall\n",
       "0  I bought two of these 8.5 fl oz hand cream, an...      2.0\n",
       "1  Believe me, over the years I have tried many, ...      5.0\n",
       "2                       Great hand lotion Five Stars      5.0\n",
       "3  This is the best for the severely dry skin on ...      5.0\n",
       "4  The best non- oily hand cream ever. It heals o...      5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need just 3 columns here, overall score, review text and summary\n",
    "#lets concatenate reviewtext and summary into a new column called final_review\n",
    "dataset['final_review'] = dataset['reviewText'] + ' ' + dataset['summary']\n",
    "#final_dataset \n",
    "dataset = dataset[['final_review', 'overall']]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_review    0.000000\n",
       "overall         0.001001\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get count of empty rows\n",
    "dataset[dataset['final_review'].isna()].count()\n",
    "#to get % of empty rows\n",
    "dataset[dataset['final_review'].isna()].count()/len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wow!!!!!!! 575 records dont have reviews but have a rating. Let's take them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()\n",
    "#let's reset the index\n",
    "dataset.reset_index(inplace = True)\n",
    "del dataset['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    0.664911\n",
       "4.0    0.122699\n",
       "1.0    0.087908\n",
       "3.0    0.073112\n",
       "2.0    0.051370\n",
       "Name: overall, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#look at the destribution of data on our target variable\n",
    "dataset['overall'].describe()\n",
    "dataset['overall'].value_counts()/len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets reclassify the dataset to just 3 classes, positive (4 & 5), neg(1&2) and neutral(3).( pos 1, neu 0 and neg -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    0.787610\n",
       "-1    0.139278\n",
       " 0    0.073112\n",
       "Name: overall, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['overall'] = dataset['overall'].apply(lambda x : 1 if x in (4,5) else -1 if x in (1,2) else 0)\n",
    "dataset['overall'].value_counts()/len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to avoid a sampling bias we'll augment the reviews by oversampling. After oversampling we'll split the dataset into test and train using stratified sampling with the StratifiedShuffleSplit module in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets check the distribution of data on the test data and see if the representation of the target variable is the close to the original sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split= StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "for train_index, test_index in split.split(dataset, dataset['overall']):\n",
    "    strat_train_set = dataset.loc[train_index]\n",
    "    strat_test_set = dataset.loc[test_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    0.787607\n",
       "-1    0.139281\n",
       " 0    0.073111\n",
       "Name: overall, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set['overall'].value_counts()/len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now to start the main work!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text augmentation to even out the trainset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install textaugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ndukwead\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ndukwead\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\ndukwead\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#import necessary nltk libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from textaugment import Wordnet, Translate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, i have embody so exhausted lately. i want to rest'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=Wordnet()\n",
    "#translate = Translate(src=\"en\", to=\"en\")\n",
    "#test\n",
    "sentence ='Hello, I have been so exhausted lately. I need to rest'\n",
    "t.augment(sentence)\n",
    "#translate.augment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augment reviews with scores less than 4\n",
    "augmented = pd.DataFrame()\n",
    "for i in strat_train_set[strat_train_set['overall'] != 1].index:\n",
    "    text_aug = t.augment(str(strat_train_set['final_review'][i]))\n",
    "    augmented = augmented.append({'final_review':text_aug ,'overall': dataset['overall'][i]} , ignore_index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented.to_csv('augmented_text.csv', index = False)\n",
    "augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 60% of records with scores equal to 1 and add augmented to train set\n",
    "\n",
    "part_na = round(strat_train_set[strat_train_set['overall'] == 1].shape[0]*0.6)\n",
    "strat_train_set.head()\n",
    "five_indices = strat_train_set[strat_train_set.overall == 1].index\n",
    "random_indices = np.random.choice(five_indices, part_na, replace= False)\n",
    "random_indices \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.drop(random_indices, inplace = True)\n",
    "strat_train_set['overall'].value_counts()/len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append augmented to strat_train_set\n",
    "strat_train_set = strat_train_set.append(augmented)\n",
    "strat_train_set['overall'].value_counts()/len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set['overall'].value_counts()/len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction\n",
    "`basically we're going to get all the words (corpus) used in the final_reviews column of the dataset and convert them to features. Each of these reviews will then record the frequency of our features and their weights (i.e. the words) \n",
    "We're using the TF-IDF vectorizer in sklearn to achieve this\n",
    "we can limit the number of features to a maximum using the most commonly occuring words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import machine learning related libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#let's import our TF-IDF module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer() #we can use to limit the number of features selected using max_features\n",
    "X_test =vectorizer.fit_transform(strat_test_set['final_review'])\n",
    "\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't add the limit to the count vectorizer yet because I wanted to find out how many unique words were in the test dataset.\n",
    "since there are just 37,583 possible features. let's use it like that. this is a sparse matrix, see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#transform X_train using test_shape\n",
    "X_train = vectorizer.transform(strat_train_set['final_review'])\n",
    "\n",
    "\n",
    "y_train = strat_train_set['overall']\n",
    "y_test = strat_test_set['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay,f1_score,precision_score,recall_score\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)  \n",
    "print('Accuracy', model.score(X_test, y_test))\n",
    "print('F1', f1_score(y_test,model.predict(X_test), average=\"macro\"))\n",
    "print('Precision', precision_score(y_test, model.predict(X_test), average=\"macro\"))\n",
    "print('Recall', recall_score(y_test, model.predict(X_test), average=\"macro\")) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "f, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
    "plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
    "axes = axes.ravel()\n",
    "for ind in range(3):\n",
    "    cm_display = ConfusionMatrixDisplay(cm[ind], display_labels=[0, model.classes_[ind]]).plot()\n",
    "    cm_display.plot(ax=axes[ind], xticks_rotation=45)\n",
    "    cm_display.ax_.set_title(name)\n",
    "    cm_display.im_.colorbar.remove()\n",
    "    cm_display.ax_.set_xlabel('')\n",
    "    if i!=0:\n",
    "        cm_display.ax_.set_ylabel('')\n",
    "    \n",
    "f.text(0.4, 0.1, 'Predicted label', ha='left')\n",
    "\n",
    "\n",
    "\n",
    "f.colorbar(cm_display.im_, ax=axes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
